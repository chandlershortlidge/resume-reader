{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40f792b",
   "metadata": {},
   "source": [
    "# PymuPDF reader\n",
    "- Read the tutorial here: https://pymupdf.readthedocs.io/en/latest/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bec2efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using PyMuPDF v1.26.3 from /Users/eliza/Desktop/resume-reader/.venv/lib/python3.12/site-packages/fitz/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# automatically reload modules, including the ones you wrote yourself\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "import pdf_processor as ppr\n",
    "\n",
    "print(f\"Using PyMuPDF v{fitz.__version__} from {fitz.__file__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7df53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PATH = \"/Users/eliza/Desktop/resume-reader/data-scientist-resume-example.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8304433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded document with 1 pages\n",
      "format: PDF 1.4\n",
      "title: \n",
      "author: \n",
      "subject: \n",
      "keywords: \n",
      "creator: \n",
      "producer: PDFShift.io\n",
      "creationDate: D:20220728203846+00'00'\n",
      "modDate: D:20220728203846+00'00'\n",
      "trapped: \n",
      "encryption: None\n"
     ]
    }
   ],
   "source": [
    "doc = ppr.load_file(RESUME_PATH)\n",
    "ppr.report_metadata(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5b893",
   "metadata": {},
   "source": [
    "### Extracting Text and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f663cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from each page\n",
    "\n",
    "for page_num in range(doc.page_count):\n",
    "    page = doc.load_page(page_num)\n",
    "    text = page.get_text()\n",
    "\n",
    "#close when done\n",
    "# doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88139888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KANDACE LOUDOR\\nDATA SCIENTIST\\nCONTACT\\nkloudor@email.com\\n(123) 456-7890\\nMount Laurel, NJ\\nLinkedIn\\nGithub\\nEDUCATION\\nB.S.\\nStatistics\\nRutgers University\\nSeptember 2011 - April 2015\\nNew Brunswick, NJ\\nSKILLS\\nPython (NumPy, Pandas,\\nScikit-learn, Keras, Flask)\\nSQL (MySQL, Postgres)\\nGit\\nTime Series Forecasting\\nProductionizing Models\\nRecommendation Engines\\nCustomer Segmentation\\nAWS\\nWORK EXPERIENCE\\nData Scientist\\nGrubhub\\nJune 2018 - current / Princeton, NJ\\nDeployed a recommendation engine to production to\\nconditionally recommend other menu items based on past order\\nhistory, increasing average order size by 7%\\nImplemented various time series forecasting techniques to\\npredict surge in orders, lowering customer wait by 10 minutes\\nDesigned a model in a pilot to increase incentives for drivers\\nduring peak hours, increasing driver availability by 22%\\nLed a team of 3 data scientist to model the ordering process 5\\nunique ways, reported results, and made recommendations to\\nincrease order output by 9%\\nData Scientist\\nSpectrix Analytical Services\\nMarch 2016 - June 2018 / Princeton, NJ\\nBuilt a customer attrition random forest model that improved\\nmonthly retention by 12 basis points for clients likely to opt-out\\nby providing relevant product features for them\\nCoordinated with the product and marketing teams to determine\\nwhat kind of client interactions resulted in maximized service\\nopt-ins, increasing conversions by 18%\\nPartnered with product team to create a production\\nrecommendation engine in Python that improved the length on-\\npage for users with $225K in incremental annual revenue\\nCompiled and analyzed data surrounding the prototypes for a\\nprosthesis, which saved over $1M in its creation\\nEntry-Level Data Analyst\\nAvenica\\nApril 2015 - March 2016 / Mount Laurel, NJ\\nCollaborated with product managers to perform cohort analysis\\nthat identified an opportunity to reduce pricing by 21% for a\\nsegment of users to boost yearly revenue by $560,000\\nConstructed operational reporting in Tableau to improve\\nscheduling contractors, saving $90,000 in the annual budget\\nImplemented a long-term pricing experiment that improved\\ncustomer lifetime value by 23%\\nRan, submitted, and reported on monthly client enrollments,\\nservices opted in for, and the employees assigned to clients\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf443d8",
   "metadata": {},
   "source": [
    "### Keyword Searching\n",
    "- To locate where a term appears on the page (useful for highlighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9076d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 0 of /Users/eliza/Desktop/resume-reader/data-scientist-resume-example.pdf\n",
      "Found 'AWS' on page 1 at:\n",
      "    Rect(152.390625, 516.9819946289062, 176.43511962890625, 531.9755859375)\n",
      "Saved highlighted PDF as: highlighted_output.pdf\n"
     ]
    }
   ],
   "source": [
    "keyword = \"AWS\"\n",
    "\n",
    "def highlight_keywords(doc: fitz.Documnt, kw_list: list[str]) -> fitz.Document:\n",
    "    ...\n",
    "\n",
    "# Loop over pages and search for your keyword\n",
    "for page_num in range(doc.page_count):\n",
    "    page = doc.load_page(page_num)\n",
    "    print(page)\n",
    "\n",
    "    matches = page.search_for(keyword)\n",
    "    if matches:\n",
    "        print(f\"Found '{keyword}' on page {page_num + 1} at:\")\n",
    "        for r in matches:\n",
    "             # print bounding box coordinates\n",
    "            print(f\"    {r}\")\n",
    "            # add a yellow highlight annotation\n",
    "            highlight = page.add_highlight_annot(r)\n",
    "\n",
    "\n",
    "# save PDF with highlights\n",
    "out_path = \"highlighted_output.pdf\"\n",
    "doc.save(out_path, garbage=4, deflate=True, clean=True)\n",
    "\n",
    "\n",
    "print(f\"Saved highlighted PDF as: {out_path}\")                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbe837",
   "metadata": {},
   "source": [
    "### get_links()\n",
    "- https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 links:\n",
      "  → URI: https://linkedin.com/\n",
      "    location on page: Rect(116.25, 190.5, 160.5, 205.5)\n",
      "  → URI: https://github.com/\n",
      "    location on page: Rect(125.25, 206.25, 160.5, 221.25)\n"
     ]
    }
   ],
   "source": [
    "# Loop through pages and inspect links\n",
    "# LET'S REWRITE IT **UNDERSTANDING** WHAT IT DOES\n",
    "# everything begins with http or htto\n",
    "for page_num in range(doc.page_count):\n",
    "    page = doc.load_page(page_num) \n",
    "    links = page.links() # list of link-dicts from page\n",
    "\n",
    "    if not links:\n",
    "        continue\n",
    "\n",
    "    print(f\"Page {page_num + 1} links:\")\n",
    "    for link in links: \n",
    "    # Common keys: 'kind', 'from', and either 'uri' or 'xref'/'to'\n",
    "        kind = link[\"kind\"]\n",
    "        location = link[\"from\"] # where the link is located\n",
    "        uri = link.get(\"uri\") # external URL, if any\n",
    "        to = link.get(\"to\") # (page, ...) for internal jumps\n",
    "\n",
    "        if uri:\n",
    "            print(f\"  -> URI: {uri}\")\n",
    "        elif to:\n",
    "            print(f\"  → Internal link to page {to[0] + 1}\")\n",
    "        else:\n",
    "            print(\"  → Other link kind:\", kind)\n",
    "\n",
    "        print(f\"    location on page: {location}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42dfcb1",
   "metadata": {},
   "source": [
    "## Convert to markdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2549635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /Users/eliza/Desktop/resume-reader/data-scientist-resume-example.pdf → output.md\n"
     ]
    }
   ],
   "source": [
    "def pdf_to_markdown(pdf_path, md_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    md_lines = []\n",
    "\n",
    "    for p in range(doc.page_count):\n",
    "        page = doc.load_page(p)\n",
    "        # get a nested dict of blocks → lines → spans\n",
    "        page_dict = page.get_text(\"dict\")\n",
    "\n",
    "        for block in page_dict[\"blocks\"]:\n",
    "            # Only text blocks (ignore images, drawings)\n",
    "            if block[\"type\"] != 0:\n",
    "                continue\n",
    "\n",
    "            # Join all spans in this block into one text string\n",
    "            text = \" \".join(span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"])\n",
    "            text = text.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            #  Simple heuristic: ALL CAPS + short → H2\n",
    "            if text.isupper() and len(text) < 60:\n",
    "                md_lines.append(f\"## {text.title()}\")\n",
    "                continue\n",
    "\n",
    "            # Detect bullet lists (e.g. lines starting with • or –)\n",
    "            if re.match(r\"^[•\\-\\u2022]\\s+\", text):\n",
    "                item = re.sub(r\"^[•\\-\\u2022]\\s+\", \"\", text)\n",
    "                md_lines.append(f\"-{item}\")\n",
    "                continue\n",
    "\n",
    "            # Otherwise treat as paragraph\n",
    "            md_lines.append(text)\n",
    "\n",
    "        # Add a page break marker (optional)\n",
    "        md_lines.append(\"\\n---\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "# Write out the Markdown\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(md_lines))\n",
    "\n",
    "    print(f\"Converted {pdf_path} → {md_path}\")\n",
    "\n",
    "# Usage\n",
    "pdf_to_markdown(RESUME_PATH, \"output.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1479c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
